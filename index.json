
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Tin Nguyen is a PhD Student at MIT. He is advised by Tamara Broderick. His research develops computational methods for inference and sensitivity detection in Bayesian models.\n","date":1705190400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1705190400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Tin Nguyen is a PhD Student at MIT. He is advised by Tamara Broderick. His research develops computational methods for inference and sensitivity detection in Bayesian models.","tags":null,"title":"Tin Nguyen","type":"authors"},{"authors":["Sameer K. Deshpande","Soumya Ghosh","Tin Nguyen","Tamara Broderick"],"categories":null,"content":"","date":1705190400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705190400,"objectID":"6adc1d9cf9d2b5badac5bcb0f3f69ca3","permalink":"https://tinnguyen96.github.io/publication/test-log-likelihood/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/test-log-likelihood/","section":"publication","summary":"Test log-likelihood is commonly used to compare different models of the same data or different approximate inference algorithms for fitting the same probabilistic model. We present simple examples demonstrating how comparisons based on test log-likelihood can contradict comparisons according to other objectives. Specifically, our examples show that (i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need not also yield more accurate posterior approximations and (ii) conclusions about forecast accuracy based on test log-likelihood comparisons may not agree with conclusions based on root mean squared error.","tags":["Evaluation"],"title":"Are you using test log-likelihood correctly?","type":"publication"},{"authors":[],"categories":null,"content":"","date":1699880400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699880400,"objectID":"403aca7705c689796a1db53d378406ba","permalink":"https://tinnguyen96.github.io/talk/sensitivity-of-mcmc-analyses-to-data-removal/","publishdate":"2023-12-01T00:00:00Z","relpermalink":"/talk/sensitivity-of-mcmc-analyses-to-data-removal/","section":"event","summary":"Contributed talk at BAYSM:O 2023","tags":["Markov Chain Monte Carlo"],"title":"Sensitivity of MCMC analyses to data removal","type":"event"},{"authors":null,"categories":null,"content":"Using Boston’s property assessment data, I built a prediction system for the price of a land parcel using its physical attributes. Stakeholders who might use the predictions are the city of Boston itself (to determine property tax) and homeowners (to find a fair value for their purchase). I used pandas to manipulate csvs, sklearn to preprocess (removing outliers, imputing missing features) and cross-validate, and finall LightGBM to fit learner. On single-household residences, the system is reasonably accurate: the average absolute percentage error is about 25%. The most important predictive feature is the total living area.\n","date":1694476800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1694476800,"objectID":"54d6c6226e6245071af1cd61957848e0","permalink":"https://tinnguyen96.github.io/project/parcel-price/","publishdate":"2023-09-12T00:00:00Z","relpermalink":"/project/parcel-price/","section":"project","summary":"Build supervised learning model and evaluate accuracy for predicting Boston property prices.","tags":["Supervised Learning","Gradient Boosting Machine","Real Estate"],"title":"Parcel Price Prediction","type":"project"},{"authors":["Kaifu Wang","Hangfeng He","Tin Nguyen","Piyush Kumar","Dan Roth"],"categories":null,"content":"","date":1685664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685664000,"objectID":"0401ddda0d7b40f0b516f0988515f432","permalink":"https://tinnguyen96.github.io/publication/regularization-and-constraints/","publishdate":"2023-06-02T00:00:00Z","relpermalink":"/publication/regularization-and-constraints/","section":"publication","summary":"Prior knowledge and symbolic rules in machine learning are often expressed in the form of label constraints, especially in structured prediction problems. In this work, we compare two common strategies for encoding label constraints in a machine learning pipeline, *regularization with constraints* and *constrained inference*, by quantifying their impact on model performance. For regularization, we show that it narrows the generalization gap by precluding models that are inconsistent with the constraints. However, its preference for small violations introduces a bias toward a suboptimal model. For constrained inference, we show that it reduces the population risk by correcting a model's violation, and hence turns the violation into an advantage. Given these differences, we further explore the use of two approaches together and propose conditions for constrained inference to compensate for the bias introduced by regularization, aiming to improve both the model complexity and optimal risk.","tags":["Statistical Learning Theory"],"title":"On regularization and inference with label constraints","type":"publication"},{"authors":["Tin Nguyen","Jonathan Huggins","Lorenzo Masoero","Lester Mackey","Tamara Broderick"],"categories":null,"content":"","date":1685404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1685404800,"objectID":"fb766b66a6fa932e70da568a9a6b7f00","permalink":"https://tinnguyen96.github.io/publication/finite-crm-approximations/","publishdate":"2023-05-30T00:00:00Z","relpermalink":"/publication/finite-crm-approximations/","section":"publication","summary":"Completely random measures (CRMs) and their normalizations (NCRMs) offer flexible models in Bayesian nonparametrics. But their infinite dimensionality presents challenges for inference. Two popular finite approximations are truncated finite approximations (TFAs) and independent finite approximations (IFAs). While the former have been well-studied, IFAs lack similarly general bounds on approximation error, and there has been no systematic comparison between the two options. In the present work, we propose a general recipe to construct practical finite-dimensional approximations for homogeneous CRMs and NCRMs, in the presence or absence of power laws. We call our construction the automated independent finite approximation (AIFA). Relative to TFAs, we show that AIFAs facilitate more straightforward derivations and use of parallel computing in approximate inference. We upper bound the approximation error of AIFAs for a wide class of common CRMs and NCRMs — and thereby develop guidelines for choosing the approximation level. Our lower bounds in key cases suggest that our upper bounds are tight. We prove that, for worst-case choices of observation likelihoods, TFAs are more efficient than AIFAs. Conversely, we find that in real-data experiments with standard likelihoods, AIFAs and TFAs perform similarly. Moreover, we demonstrate that AIFAs can be used for hyperparameter estimation even when other potential IFA options struggle or do not apply.","tags":["Bayesian Statistics","Nonparametric","Unsupervised Learning","Topic Models"],"title":"Independent finite approximations for Bayesian nonparametric inference","type":"publication"},{"authors":[],"categories":null,"content":"","date":1670054400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670054400,"objectID":"4cb47349c1238b9566ec957e6eeecb24","permalink":"https://tinnguyen96.github.io/talk/are-you-using-test-log-likelihood-correctly/","publishdate":"2022-12-10T00:00:00Z","relpermalink":"/talk/are-you-using-test-log-likelihood-correctly/","section":"event","summary":"Poster presentation at ICBINB Workshop at NeurIPS 2022","tags":[],"title":"Are you using test log-likelihood correctly?","type":"event"},{"authors":[],"categories":null,"content":"","date":1656423000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656423000,"objectID":"8e0c09303fcbaa7f6c74569b964a5c51","permalink":"https://tinnguyen96.github.io/talk/many-processors-little-time-mcmc-for-partitions-via-optimal-transport-couplings/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/talk/many-processors-little-time-mcmc-for-partitions-via-optimal-transport-couplings/","section":"event","summary":"Invited talk at ISBA 2022","tags":[],"title":"Many processors, little time: MCMC for partitions via optimal transport couplings","type":"event"},{"authors":[],"categories":null,"content":"","date":1648540800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648540800,"objectID":"423a43a491fefbd7129960969af80ab2","permalink":"https://tinnguyen96.github.io/talk/many-processors-little-time-mcmc-for-partitions-via-optimal-transport-couplings/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/talk/many-processors-little-time-mcmc-for-partitions-via-optimal-transport-couplings/","section":"event","summary":"Oral presentation at AISTATS 2022","tags":[],"title":"Many processors, little time: MCMC for partitions via optimal transport couplings","type":"event"},{"authors":["Tin Nguyen","Brian L. Trippe","Tamara Broderick"],"categories":null,"content":"","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"f3e79cc246c9f0cb7fc936d6f2d74689","permalink":"https://tinnguyen96.github.io/publication/coupling-crp-chains/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/coupling-crp-chains/","section":"publication","summary":"Markov chain Monte Carlo (MCMC) methods are often used in clustering since they guarantee asymptotically exact expectations in the infinite-time limit. In finite time, though, slow mixing often leads to poor performance. Modern computing environments offer massive parallelism, but naive implementations of parallel MCMC can exhibit substantial bias. In MCMC samplers of continuous random variables, Markov chain couplings can overcome bias. But these approaches depend crucially on paired chains meetings after a small number of transitions. We show that straightforward applications of existing coupling ideas to discrete clustering variables fail to meet quickly. This failure arises from the ``label-switching problem'', as semantically equivalent cluster relabelings impede fast meeting of coupled chains. We instead consider chains as exploring the space of partitions rather than partitions' (arbitrary) labelings. Using a metric on the partition space, we formulate a practical algorithm using optimal transport couplings. Our theory confirms our method is accurate and efficient. In experiments ranging from clustering of genes or seeds to graph colorings, we show the benefits of our coupling in the highly parallel, time-limited regime.","tags":["Bayesian Statistics","Markov Chain Monte Carlo","Unsupervised Learning"],"title":"Many processors, little time: MCMC for partitions via optimal transport couplings","type":"publication"},{"authors":["William T. Stephenson","Soumya Ghosh","Tin Nguyen","Mikhail Yurochkin","Sameer K. Deshpande","Tamara Broderick"],"categories":null,"content":"","date":1646092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646092800,"objectID":"614d4243d5c3b60f1b480272785708a8","permalink":"https://tinnguyen96.github.io/publication/gp-robustness/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/gp-robustness/","section":"publication","summary":"Gaussian processes (GPs) are used to make medical and scientific decisions, including in cardiac care and monitoring of atmospheric carbon dioxide levels. Notably, the choice of GP kernel is often somewhat arbitrary. In particular, uncountably many kernels typically align with qualitative prior knowledge (e.g.\\ function smoothness or stationarity). But in practice, data analysts choose among a handful of convenient standard kernels (e.g.\\ squared exponential). In the present work, we ask:Would decisions made with a GP differ under other, qualitatively interchangeable kernels? We show how to answer this question by solving a constrained optimization problem over a finite-dimensional space. We can then use standard optimizers to identify substantive changes in relevant decisions made with a GP. We demonstrate in both synthetic and real-world examples that decisions made with a GP can exhibit non-robustness to kernel choice, even when prior draws are qualitatively interchangeable to a user.","tags":["Bayesian Statistics","Gaussian Process","Robustness"],"title":"Measuring the robustness of Gaussian processes to kernel choice","type":"publication"},{"authors":["William T. Stephenson","Soumya Ghosh","Tin Nguyen","Sameer K. Deshpande","Tamara Broderick"],"categories":null,"content":"","date":1607731200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607731200,"objectID":"3ddb18f79fbfdac08569676a05128b67","permalink":"https://tinnguyen96.github.io/publication/approx-cv/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/approx-cv/","section":"publication","summary":"Many modern data analyses benefit from explicitly modeling dependence structure in data -- such as measurements across time or space, ordered words in a sentence, or genes in a genome. A gold standard evaluation technique is structured cross-validation (CV), which leaves out some data subset (such as data within a time interval or data in a geographic region) in each fold. But CV here can be prohibitively slow due to the need to re-run already-expensive learning algorithms many times. Previous work has shown approximate cross-validation (ACV) methods provide a fast and provably accurate alternative in the setting of empirical risk minimization. But this existing ACV work is restricted to simpler models by the assumptions that (i) data across CV folds are independent and (ii) an exact initial model fit is available. In structured data analyses, both these assumptions are often untrue. In the present work, we address (i) by extending ACV to CV schemes with dependence structure between the folds. To address (ii), we verify -- both theoretically and empirically -- that ACV quality deteriorates smoothly with noise in the initial fit. We demonstrate the accuracy and computational benefits of our proposed methods on a diverse set of real-world applications.","tags":["Hidden Markov Models","Conditional Random Fields","Cross-Validation"],"title":"Approximate cross-validation for structured models","type":"publication"},{"authors":[],"categories":null,"content":"","date":1605618000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605618000,"objectID":"fdae6fcc567e0dadccb88b165537bbf0","permalink":"https://tinnguyen96.github.io/talk/independent-finite-approximations-for-bayesian-nonparametric-inference/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/talk/independent-finite-approximations-for-bayesian-nonparametric-inference/","section":"event","summary":"Contributed talk at BAYSM:O 2020","tags":[],"title":"Independent finite approximations for Bayesian nonparametric inference","type":"event"},{"authors":["Brian L. Trippe","Tin Nguyen","Tamara Broderick"],"categories":null,"content":"","date":1601510400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601510400,"objectID":"744c306dd954c13d7da4841d139c64a2","permalink":"https://tinnguyen96.github.io/publication/coupling-crp-chains-aabi/","publishdate":"2020-10-01T00:00:00Z","relpermalink":"/publication/coupling-crp-chains-aabi/","section":"publication","summary":"Computational couplings of Markov chains provide a practical route to unbiased Monte Carlo estimation that can utilize parallel computation. However, these approaches depend crucially on chains meeting after a small number of transitions. For models that assign data into groups, e.g. mixture models, the obvious approaches to couple Gibbs samplers fail to meet quickly. This failure owes to the so-called \"label-switching\" problem; semantically equivalent relabelings of the groups contribute well-separated posterior modes that impede fast mixing and cause large meeting times. We here demonstrate how to avoid label switching by considering chains as exploring the space of partitions rather than labelings. Using a metric on this space, we employ an optimal transport coupling of the Gibbs conditionals. This coupling outperforms alternative couplings that rely on labelings and, on a real dataset, provides estimates more precise than usual ergodic averages in the limited time regime. Code is available at github.com/tinnguyen96/coupling-Gibbs-partition.","tags":["Bayesian Statistics","Markov Chain Monte Carlo"],"title":"Optimal transport couplings of Gibbs samplers on partitions for unbiased estimation","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://tinnguyen96.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Tin Nguyen","Samory Kpotufe"],"categories":null,"content":"","date":1544572800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544572800,"objectID":"733bdbb96df332efc9c7c81dadff6212","permalink":"https://tinnguyen96.github.io/publication/pac-bayes-tree/","publishdate":"2022-12-01T00:00:00Z","relpermalink":"/publication/pac-bayes-tree/","section":"publication","summary":"We present a weighted-majority classification approach over subtrees of a fixed tree, which provably achieves excess-risk of the same order as the best tree-pruning. Furthermore, the computational efficiency of pruning is maintained at both training and testing time despite having to aggregate over an exponential number of subtrees. We believe this is the first subtree aggregation approach with such guarantees.","tags":["Supervised Learning","Statistical Learning Theory","Decision Trees"],"title":"PAC-Bayes Tree: weighted subtrees with guarantees","type":"publication"}]